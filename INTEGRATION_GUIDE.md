# Руководство по интеграции Whisper с другими проектами

Данное руководство содержит информацию, необходимую для интеграции микросервиса Whisper с гибридным распознаванием речи в другие проекты.

## Необходимая информация для интеграции

### 1. Интерфейс существующего механизма распознавания речи

#### Какой API/интерфейс использует текущий механизм распознавания речи?
Микросервис Whisper предоставляет REST API для распознавания речи. Основные эндпоинты:
- `/api/v1/hybrid/stt` - гибридное распознавание речи с автоматическим переключением между локальной и внешней обработкой
- `/api/v1/voice/verify` - распознавание речи с верификацией говорящего

#### Какие методы/функции вызываются для обработки аудио?
Основные внутренние функции для распознавания речи:
- `process_audio_hybrid(audio_path, verify_speaker_flag, return_debug)` - метод для гибридной обработки аудио
- `transcribe_audio(audio_path, detailed)` - метод для локального распознавания через Whisper
- `process_audio_remote(audio_path, verify_speaker)` - метод для обработки через внешний API

#### Какой формат входных и выходных данных ожидается?

**Входные данные:**
- Аудиофайлы в форматах WAV, MP3, OGG, FLAC
- Поддерживается отправка как файла, так и URL на аудиофайл
- Дополнительные параметры: верификация говорящего, использование семантической валидации и пороговые значения

**Выходные данные:**
```json
{
  "source": "local",  // или "remote"
  "text": "распознанный текст",
  "metadata": {
    "confidence": 0.93,  // уверенность распознавания
    "speaker_match": 0.97,  // соответствие голосовому отпечатку (если применимо)
    "duration": 7.5,  // длительность аудио в секундах
    "language": "ru",  // определенный язык
    "fallback_used": false,  // был ли использован резервный метод
    "semantic_diff": 0.12  // семантическая разница между результатами (если применимо)
  }
}
```

### 2. Архитектурные особенности проекта

#### Это монолитное приложение или микросервисная архитектура?
Whisper реализован как отдельный микросервис, который может быть интегрирован в более крупную микросервисную архитектуру или использоваться как самостоятельный сервис.

#### Какой язык программирования используется в проекте?
Проект написан на Python 3.12+ с использованием следующих основных технологий:
- FastAPI для REST API
- PyTorch и OpenAI Whisper для распознавания речи
- SpeechBrain и Resemblyzer для верификации голоса
- Sentence-Transformers для семантической валидации (опционально)

#### Какие механизмы обмена данными поддерживаются?
- REST API (основной метод взаимодействия)
- CLI-интерфейс для локального тестирования и интеграции через скрипты
- Возможность расширения для поддержки других протоколов (WebSockets, gRPC)

### 3. Требования к производительности

#### Нужна ли обработка в реальном времени или допустима асинхронная обработка?
Текущая реализация поддерживает асинхронную обработку аудиофайлов. Обработка в реальном времени (потоковая обработка аудио) не реализована напрямую, но может быть добавлена как расширение.

#### Какие ограничения по времени отклика?
- Локальное распознавание: в среднем 0.5-2 секунды на 10 секунд аудио (зависит от модели и наличия GPU)
- Гибридное распознавание: дополнительная задержка 1-5 секунд при переключении на внешний API
- Настраиваемый таймаут для локального распознавания (по умолчанию 5 секунд)

#### Ожидаемая нагрузка (запросов в секунду/минуту)?
- На CPU: до 10 запросов в минуту (модель base)
- На GPU: до 60 запросов в минуту (модель base)
- Масштабирование через развертывание нескольких инстансов с балансировкой нагрузки

### 4. Инфраструктурные детали

#### Где размещается проект?
Проект может быть развернут:
- Локально (для разработки и тестирования)
- На серверах компании (on-premise)
- В облачных средах (AWS, GCP, Azure)
- С использованием Docker и Kubernetes для оркестрации

#### Доступны ли GPU для ускорения распознавания речи?
Проект поддерживает как CPU, так и GPU для ускорения распознавания:
- Для GPU: поддержка CUDA через PyTorch
- В docker-compose.yml предусмотрена конфигурация для подключения GPU
- Автоматическое использование GPU, если он доступен

#### Есть ли ограничения по использованию памяти/дискового пространства?
- Память: минимум 4 ГБ RAM (для модели tiny/base), рекомендуется 8+ ГБ
- Для модели large: минимум 16 ГБ RAM
- Дисковое пространство: ~2 ГБ для моделей и зависимостей
- Дополнительное пространство для хранения аудиофайлов и голосовых отпечатков (зависит от использования)

## Варианты интеграции

### 1. API-интеграция
Наиболее простой способ интеграции - использование REST API. Примеры запросов:

```bash
# Гибридное распознавание речи
curl -X POST \
  http://whisper-service:8000/api/v1/hybrid/stt \
  -H "X-API-Key: your-api-key" \
  -F "audio_file=@./command.wav" \
  -F "verify_speaker=false" \
  -F "use_semantics=true" \
  -F "semantic_threshold=0.8"
```

### 2. Библиотечная интеграция
Для более тесной интеграции можно использовать внутренние компоненты как библиотеку:

```python
from whisper.app.hybrid.controller import process_audio_hybrid
from whisper.app.audio.processor import process_audio_file
from pathlib import Path

async def recognize_speech(audio_path):
    processed_audio = await process_audio_file(Path(audio_path))
    result = await process_audio_hybrid(
        audio_path=processed_audio,
        verify_speaker_flag=False,
        return_debug=True
    )
    return result
```

### 3. Контейнерная интеграция
Интеграция через Docker и Docker Compose:

```yaml
# docker-compose.yml
services:
  your_service:
    # Конфигурация вашего сервиса
    depends_on:
      - whisper
  
  whisper:
    image: whisper
    # Конфигурация из docker-compose.yml проекта Whisper
```

## Контрольный список для интеграции

1. Выбрать метод интеграции (API, библиотека, контейнеры)
2. Настроить конфигурацию (`config.yaml`) под требования проекта
3. Обеспечить доступ к аудиофайлам
4. Определить стратегию обработки ошибок и резервный путь
5. Настроить мониторинг и логирование
6. Провести тестирование производительности и интеграционное тестирование
7. Настроить CI/CD для автоматического развертывания

## Контакты для поддержки

По вопросам интеграции и технической поддержки обращайтесь к команде разработки:
- Email: support@elrise.whisper.example.com
- Внутренний канал: #whisper-integration
